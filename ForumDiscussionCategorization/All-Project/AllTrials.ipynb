{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10171207,"sourceType":"datasetVersion","datasetId":6281771},{"sourceId":10174634,"sourceType":"datasetVersion","datasetId":6284374},{"sourceId":10180429,"sourceType":"datasetVersion","datasetId":6288538}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional,Input,GlobalMaxPooling1D,BatchNormalization,Concatenate,Conv1D\nimport gensim\nimport re\nimport nltk\nimport string\nimport torch\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertConfig,BertModel\nfrom transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange  \nfrom nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk.tree import Tree\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.utils import shuffle\nnltk.download('punkt_tab')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger_eng')\n#!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:07:12.530759Z","iopub.execute_input":"2024-12-24T20:07:12.531101Z","iopub.status.idle":"2024-12-24T20:07:12.543349Z","shell.execute_reply.started":"2024-12-24T20:07:12.531068Z","shell.execute_reply":"2024-12-24T20:07:12.542400Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/traintest/train.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:21:05.668245Z","iopub.execute_input":"2024-12-24T20:21:05.668884Z","iopub.status.idle":"2024-12-24T20:21:05.771588Z","shell.execute_reply.started":"2024-12-24T20:21:05.668845Z","shell.execute_reply":"2024-12-24T20:21:05.770682Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"       SampleID                                         Discussion  \\\n0             1  Without sitting down and doing it manually, yo...   \n1             2               All your Search ends with this link.   \n2             3  No, the program you're using is made to be com...   \n3             4  Mike Woicik\\n\\nThe correct answer is: Mike Woi...   \n4             5  No, but not because of why you might think. Wh...   \n...         ...                                                ...   \n24984     24985  He's got more pull with the horses than most j...   \n24985     24986  Yes he did for a big juicy cheeseburger with f...   \n24986     24987                                           I'm not.   \n24987     24988  It is sexual harassment because it is offensiv...   \n24988     24989  Ok it wasnt absinthe i never saw the movie i w...   \n\n               Category  \n0                Sports  \n1                  STEM  \n2                  STEM  \n3                Sports  \n4              Politics  \n...                 ...  \n24984            Sports  \n24985  Market & Economy  \n24986  Market & Economy  \n24987          Politics  \n24988             Media  \n\n[24989 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SampleID</th>\n      <th>Discussion</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Without sitting down and doing it manually, yo...</td>\n      <td>Sports</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>All your Search ends with this link.</td>\n      <td>STEM</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>No, the program you're using is made to be com...</td>\n      <td>STEM</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Mike Woicik\\n\\nThe correct answer is: Mike Woi...</td>\n      <td>Sports</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>No, but not because of why you might think. Wh...</td>\n      <td>Politics</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24984</th>\n      <td>24985</td>\n      <td>He's got more pull with the horses than most j...</td>\n      <td>Sports</td>\n    </tr>\n    <tr>\n      <th>24985</th>\n      <td>24986</td>\n      <td>Yes he did for a big juicy cheeseburger with f...</td>\n      <td>Market &amp; Economy</td>\n    </tr>\n    <tr>\n      <th>24986</th>\n      <td>24987</td>\n      <td>I'm not.</td>\n      <td>Market &amp; Economy</td>\n    </tr>\n    <tr>\n      <th>24987</th>\n      <td>24988</td>\n      <td>It is sexual harassment because it is offensiv...</td>\n      <td>Politics</td>\n    </tr>\n    <tr>\n      <th>24988</th>\n      <td>24989</td>\n      <td>Ok it wasnt absinthe i never saw the movie i w...</td>\n      <td>Media</td>\n    </tr>\n  </tbody>\n</table>\n<p>24989 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"## Normal Preprocessing","metadata":{}},{"cell_type":"code","source":"# df['Discussion'].dropna()\n# category_mapping = {\n#     'Politics': 0,\n#     'Sports': 1,\n#     'Media': 2,\n#     'Market & Economy': 3,\n#     'STEM': 4\n# }\n# df['Discussion'] = df['Discussion'].astype(str)\n# df['Discussion'] = df['Discussion'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', ' ', x))\n# df['Category'] = df['Category'].map(category_mapping)\n# df=df.drop('SampleID',axis=1)\n# nltk.download('stopwords')\n# stop_words = set(stopwords.words('english'))\n# df['Discussion'] = df['Discussion'].apply(lambda text: ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_words]))\n# df['Discussion'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FOR BERT","metadata":{}},{"cell_type":"code","source":"df['Discussion'].dropna()\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9.,!?;\\\\'\\\"\\\\s]\", \" \", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"\\d+\", \"<NUMBER>\", text))\ndf['Category'] = df['Category'].map(category_mapping)\ndf=df.drop('SampleID',axis=1)\n#stop_words = set(stopwords.words('english'))\n#df['Discussion'] = df['Discussion'].astype(str).apply(lambda text: re.sub(r'[^a-zA-Z\\s]', ' ', text))\n#df['Discussion'] = df['Discussion'].apply(lambda text: ' '.join(filter(lambda word: word.lower() not in stop_words, text.split())))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Augmentation","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import wordnet\n\ndef synonym_replacement(row):\n    text = row['Discussion']\n    words = text.split()\n    augmented_text = []\n    for word in words:\n        # Get synonyms for each word\n        synonyms = wordnet.synsets(word)\n        if synonyms:\n            # Replace the word with a synonym\n            synonym = synonyms[0].lemmas()[0].name()\n            augmented_text.append(synonym)\n        else:\n            augmented_text.append(word)\n    return {'augmented': augmented_text, 'category': row['Category']}\n\nrandom_sample = df.sample(n=5000, random_state=42)  \naugmented_data = random_sample.apply(synonym_replacement, axis=1)\naugmented_df = pd.DataFrame(augmented_data.tolist())\naugmented_df['augmented'] = augmented_df['augmented'].apply(lambda x: \" \".join(x))\naugmented_df.rename(columns={'augmented': 'Discussion', 'category': 'Category'}, inplace=True)\ndf = pd.concat([df, augmented_df], ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.concat([df, augmented_df], ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Aggressive preprocessing","metadata":{}},{"cell_type":"code","source":"##############Aggressive Preprocessing##################\ndef preprocess_text(text):\n    lemmatizer = WordNetLemmatizer()\n    # Remove NaN values\n    if pd.isnull(text):\n        return \"\"\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    # Tokenize and remove stopwords\n    words = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n\n    words = [word for word in words if word not in stop_words]\n    # Lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words]\n    # Join back into a single string\n    return ' '.join(words)\n\n# Apply preprocessing to the 'Discussion' column\ndf['Discussion']=df['Discussion'].astype(str)\ndf['Discussion'] = df['Discussion'].dropna().apply(preprocess_text)\n\n# Map categories\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Category'] = df['Category'].map(category_mapping)\n\n# Drop the 'SampleID' column\ndf = df.drop('SampleID', axis=1)\ndf['Discussion'][0]\n# Display the preprocessed DataFrame\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"#####Tokenization########\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 30000\noov_token = \"<OOV>\"\npadding_type = \"post\"\ntrunc_type = \"post\"\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\ntokenizer.fit_on_texts(df['Discussion'])\ntraining_sequences = tokenizer.texts_to_sequences(df['Discussion'])\ntraining_padded = pad_sequences(training_sequences, maxlen=100, padding=padding_type, truncating=trunc_type)\nX=np.array(training_padded)\nlabels = np.array(df['Category'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(X, labels, test_size=0.2, random_state=42,stratify=labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-trained Embedding","metadata":{}},{"cell_type":"code","source":"# # Function to load GloVe vectors\n# def load_glove_vectors(filepath):\n#     embeddings_index = {}\n#     with open(filepath, encoding=\"utf8\") as f:\n#         for line in f:\n#             values = line.split()\n#             word = values[0]\n#             vector = np.asarray(values[1:], dtype=\"float32\")\n#             embeddings_index[word] = vector\n#     return embeddings_index\n\n# # Load GloVe embeddings\n# glove_path = \"/kaggle/input/glove-300d/glove.6B.300d.txt\"\n# glove_vectors = load_glove_vectors(glove_path)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Parameters\n# vocab_size = len(tokenizer.word_index) + 1\n# embedding_dim = 300\n\n# # Initialize embedding matrix\n# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n# # Populate embedding matrix\n# for word, i in tokenizer.word_index.items():\n#     if word in glove_vectors:\n#         embedding_matrix[i] = glove_vectors[word]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nmodel=KeyedVectors.load_word2vec_format(\"/kaggle/input/google-news-300/GoogleNews-vectors-negative300.bin\", binary=True)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# embedding_dim = 300\n# word_index = tokenizer.word_index\n# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n# for word, i in word_index.items():\n#     if i < vocab_size and word in model.key_to_index:\n#         embedding_matrix[i] = model[word]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##############################cond2#################\nvocab_size = len(tokenizer.word_index) + 1\n\nembedding_dim = 300\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in tokenizer.word_index.items():\n    try:\n        embedding_matrix[i] = model[word]\n    except KeyError:\n        pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"## BILSTM","metadata":{}},{"cell_type":"code","source":"##########################BILSTM################71.444score####30000vocab####300 shape ,input_length100 ####\n##################71.5+ score #### Model1 Archi######\nmodel2 = Sequential([\n    Input(shape=(100,),dtype=np.int32),\n    Embedding(input_dim=vocab_size,\n              output_dim=embedding_dim,\n              weights=[embedding_matrix],\n              input_length=100,\n              trainable=False), \n    Bidirectional(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n    GlobalAveragePooling1D(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.1),\n    Dense(5, activation='softmax')  \n])\nmodel2.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',  \n              metrics=['accuracy'])\nmodel2.summary()\nhistory = model2.fit(\n    x_train, \n    y_train,\n    validation_data=(x_val, y_val),\n    epochs=10,  \n    batch_size=128,\n    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##########Fine Tuning##############\nfor layer in model2.layers:\n    if isinstance(layer, Embedding):\n        layer.trainable = True\nhistory = model2.fit(\n    x_train, \n    y_train,\n    validation_data=(x_val, y_val),\n    epochs=10,  \n    batch_size=128,\n    callbacks=[\n        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n        LearningRateScheduler(lambda epoch, lr: lr * 0.1 if epoch > 5 else lr)\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2.evaluate(x_val,y_val)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GRU","metadata":{}},{"cell_type":"code","source":"########################Archi2#################\nmodel4=Sequential([\n    Input(shape=(100,),dtype=np.int32),\n    Embedding(\n    input_dim=vocab_size,\n    output_dim=embedding_dim,\n    weights=[embedding_matrix],\n    input_length=100,  # Adjust the input length to match your data\n    trainable=False),\n    GRU(256,return_sequences=True),\n    Dropout(0.3),\n    GlobalMaxPooling1D(),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(5, activation='softmax')\n    \n])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model4.summary()\nmodel4.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistoryy=model4.fit(x_train, \n    y_train,\n    validation_data=(x_val, y_val),\n    epochs=10,  \n    batch_size=128,\n    callbacks=[\n        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for layer in model4.layers:\n    if isinstance(layer, Embedding):\n        layer.trainable = True\nhistory2 = model4.fit(\n    x_train, \n    y_train,\n    validation_data=(x_val, y_val),\n    epochs=10,  \n    batch_size=128,\n    callbacks=[\n        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n        LearningRateScheduler(lambda epoch, lr: lr * 0.1 if epoch > 5 else lr)\n    ]\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model4.evaluate(x_val,y_val)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## BERT","metadata":{}},{"cell_type":"code","source":"try:\n  import transformers\nexcept:\n  print(\"Installing transformers\")\n  !pip -q install transformers","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentences = df.Discussion.values\n\n# Adding CLS and SEP tokens at the beginning and end of each sentence for BERT\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = df.Category.values","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\ntry:\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n    print(\"Tokenizer downloaded successfully.\")\nexcept Exception as e:\n    print(\"An error occurred while downloading the tokenizer.\")\n    print(str(e))\n    import traceback\n    print(traceback.format_exc())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n# In the original paper, the authors used a length of 512.\nMAX_LEN = 128\n\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use train_test_split to split our data into train and validation sets for training\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n                                                            random_state=2018, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2018, test_size=0.1)\n# Torch tensors are the required datatype for our model\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initializing a BERT bert-base-uncased style configuration\nfrom transformers import BertModel, BertConfig\nconfiguration = BertConfig()\n\n# Initializing a model from the bert-base-uncased style configuration\nmodel = BertModel(configuration)\n\n# Accessing the model configuration\nconfiguration = model.config\n\nprint(configuration)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Define the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\nmodel = nn.DataParallel(model)\nmodel.to(device)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This code is taken from:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n\n# Don't apply weight decay to any parameters whose names include these tokens.\n# (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.weight']\n# Separate the `weight` parameters from the `bias` parameters.\n# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01.\n# - For the `bias` parameters, the 'weight_decay_rate' is 0.0.\noptimizer_grouped_parameters = [\n    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.1},\n\n    # Filter for parameters which *do* include those.\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n# Note - `optimizer_grouped_parameters` only includes the parameter values, not\n# the names.","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying a sample of the parameter_optimizer:  layer 3\nlayer_parameters = [p for n, p in model.named_parameters() if 'layer.3' in n]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying names of parameters for which weight decay is not applied\nno_decay","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying the list of the two dictionaries\nsmall_sample = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)][:2],\n     'weight_decay_rate': 0.1},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)][:2],\n     'weight_decay_rate': 0.0}\n]\n\nfor i, group in enumerate(small_sample):\n    print(f\"Group {i+1}:\")\n    print(f\"Weight decay rate: {group['weight_decay_rate']}\")\n    for j, param in enumerate(group['params']):\n        print(f\"Parameter {j+1}: {param}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#optimizer = BertAdam(optimizer_grouped_parameters,\n#                      lr=2e-5,\n#                      warmup=.1)\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 3\n#optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)\noptimizer = AdamW(optimizer_grouped_parameters,\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                 )\n# Total number of training steps is number of batches * number of epochs.\n# `train_dataloader` contains batched data so `len(train_dataloader)` gives\n# us the number of batches.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Creating the Accuracy Measurement Function\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import trange\nimport torch\n\n# Initialize lists to track metrics\ntrain_loss_set = []\n\n# Training loop over epochs\nfor epoch in trange(epochs, desc=\"Epoch\"):\n\n    # Set the model to training mode\n    model.train()\n\n    # Initialize training loss\n    tr_loss = 0\n    nb_tr_steps = 0\n\n    for step, batch in enumerate(train_dataloader):\n        # Move batch data to the specified device\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Clear out gradients\n        optimizer.zero_grad()\n\n        # Forward pass with loss computation\n        outputs = model(input_ids=b_input_ids, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n\n        # Ensure loss is reduced to a scalar\n        loss = outputs.loss\n        if loss.dim() > 0:  # If loss has multiple elements, reduce it\n            loss = loss.mean()\n\n        # Append the scalar loss for tracking\n        train_loss_set.append(loss.item())\n\n        # Backward pass for gradients\n        loss.backward()\n\n        # Update model parameters\n        optimizer.step()\n\n        # Update the learning rate\n        scheduler.step()\n\n        # Accumulate training loss\n        tr_loss += loss.item()\n        nb_tr_steps += 1\n\n    # Print average training loss for the epoch\n    print(f\"Train loss: {tr_loss / nb_tr_steps:.4f}\")\n\n    # Validation phase\n    model.eval()\n\n    # Initialize validation metrics\n    eval_loss = 0\n    eval_accuracy = 0\n    nb_eval_steps = 0\n\n    for batch in validation_dataloader:\n        # Move batch data to the specified device\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Disable gradient computation for evaluation\n        with torch.no_grad():\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits  # Extract logits\n\n        # Move logits and labels to CPU for metric calculation\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.cpu().numpy()\n\n        # Compute accuracy for the current batch\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n\n    # Print validation accuracy for the epoch\n    print(f\"Validation Accuracy: {eval_accuracy / nb_eval_steps:.4f}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test submission for BERT","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/traintest/test.csv')\n\ntest_df['Discussion']=test_df['Discussion'].astype(str)\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE))\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9.,!?;\\\\'\\\"\\\\s]\", \" \", text))\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"\\d+\", \"<NUMBER>\", text))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###same tokenizer###\n# Add CLS and SEP tokens to each sentence in the test data\ntest_sentences = test_df['Discussion'].values\ntest_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]\n\n# Tokenize the test sentences using the same tokenizer\ntest_tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\n\n# Convert tokens to input IDs\ntest_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in test_tokenized_texts]\n\n# Pad and truncate sequences to MAX_LEN\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\ntest_attention_masks = []\nfor seq in test_input_ids:\n    seq_mask = [float(i > 0) for i in seq]\n    test_attention_masks.append(seq_mask)\n\n# Convert to Torch tensors\ntest_inputs = torch.tensor(test_input_ids)\ntest_masks = torch.tensor(test_attention_masks)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predictions = model.predict(X_test)\n# predictions\n# Create TensorDataset and DataLoader for test data\ntest_data = TensorDataset(test_inputs, test_masks)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predicted_class = predictions.argmax(axis=-1)\n# predicted_class\n# Set the model to evaluation mode\nmodel.eval()\n\n# Store predictions\npredictions = []\n\n# Predict\nwith torch.no_grad():\n    for batch in test_dataloader:\n        # Move inputs to the same device as the model\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask = batch\n\n        # Forward pass\n        outputs = model(b_input_ids, attention_mask=b_input_mask)\n\n        # Get the logits and move to CPU\n        logits = outputs.logits.detach().cpu().numpy()\n\n        # Use argmax to get the predicted label for each instance\n        batch_predictions = np.argmax(logits, axis=1)\n        predictions.extend(batch_predictions)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(predictions)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit = pd.DataFrame({\n    \"SampleID\": test_df[\"SampleID\"],  \n    \"Category\": predictions           \n})\nprint(submit.head())\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit.to_csv(\"/kaggle/working/bfinal.csv\", index = False)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test submission","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/traintest/test.csv')\ndf['Discussion']=df['Discussion'].astype(str)\ntest_df['Discussion'] = test_df['Discussion'].dropna().apply(preprocess_text)\n# test_df['Discussion'] = test_df['Discussion'].astype(str)\n# test_df['Discussion'] = test_df['Discussion'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', ' ', x))\n# test_df['Discussion'] = test_df['Discussion'].apply(lambda text: ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_words]))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequences_test = tokenizer.texts_to_sequences(test_df['Discussion'])\npadded_sequences_test = pad_sequences(sequences_test, maxlen=100, padding='post')\n\nX_test = np.array(padded_sequences_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model2.predict(X_test)\npredictions\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_class = predictions.argmax(axis=-1)\npredicted_class","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(predicted_class)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit = pd.DataFrame({\n    \"SampleID\": test_df[\"SampleID\"],  \n    \"Category\": predicted_class  \n})\nsubmit.to_csv(\"/kaggle/working/bilstmagrglo.csv\", index = False)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# RoBERTa","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm import trange\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\n\n\n# Preprocessing\ndf['Discussion'].dropna(inplace=True)\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9.,!?;\\\\'\\\"\\\\s]\", \" \", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"\\d+\", \"<NUMBER>\", text))\ndf['Category'] = df['Category'].map(category_mapping)\ndf = df.drop('SampleID', axis=1)\n# stop_words = set(stopwords.words('english'))\n# df['Discussion'] = df['Discussion'].apply(\n#     lambda text: ' '.join(filter(lambda word: word.lower() not in stop_words, text.split()))\n# )\n\nsentences = df.Discussion.values\nlabels = df.Category.values\n\n# Tokenization using RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Tokenize the sentences and pad sequences\nMAX_LEN = 128\ninput_ids = []\nattention_masks = []\n\nfor sent in sentences:\n    encoded_dict = tokenizer.encode_plus(\n        sent,\n        add_special_tokens=True,  # Add [CLS] and [SEP]\n        max_length=MAX_LEN,       # Pad or truncate\n        padding='max_length',\n        return_attention_mask=True,\n        return_tensors='pt',      # Return pytorch tensors\n        truncation=True\n    )\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Split the dataset into training and validation\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, labels, random_state=2021, test_size=0.1\n)\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks, input_ids, random_state=2021, test_size=0.1\n)\n\n# Create DataLoader\nbatch_size = 32\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n# Load RoBERTa model for sequence classification\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"roberta-base\",\n    num_labels=5,  # Number of categories\n    output_attentions=False,\n    output_hidden_states=False,\n)\nmodel.to(device)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nfrom transformers import get_scheduler\n\nepochs = 4\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_scheduler(\n    \"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=int(total_steps * 0.1),  # 10% warm-up steps\n    num_training_steps=total_steps,\n)\n\n# Accuracy calculation\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Training loop\nfor epoch in trange(epochs, desc=\"Epoch\"):\n    # Training phase\n    model.train()\n    total_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()\n\n        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n\n    # Validation phase\n    model.eval()\n    eval_accuracy = 0\n    nb_eval_steps = 0\n\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.cpu().numpy()\n\n        eval_accuracy += flat_accuracy(logits, label_ids)\n        nb_eval_steps += 1\n\n    print(f\"Validation Accuracy: {eval_accuracy / nb_eval_steps:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_scheduler\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom tqdm import trange\nimport numpy as np\nimport re\n\n# Preprocessing\ndf['Discussion'].dropna(inplace=True)\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9.,!?;\\\\'\\\"\\\\s]\", \" \", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"\\d+\", \"<NUMBER>\", text))\ndf['Category'] = df['Category'].map(category_mapping)\ndf = df.drop('SampleID', axis=1)\n\nsentences = df.Discussion.values\nlabels = df.Category.values\n\n# Tokenization using RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Tokenize the sentences and pad sequences\nMAX_LEN = 128\ninput_ids = []\nattention_masks = []\n\nfor sent in sentences:\n    encoded_dict = tokenizer.encode_plus(\n        sent,\n        add_special_tokens=True,\n        max_length=MAX_LEN,\n        padding='max_length',\n        return_attention_mask=True,\n        return_tensors='pt',\n        truncation=True\n    )\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Split the dataset into training and validation\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, labels, random_state=2021, test_size=0.1\n)\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks, input_ids, random_state=2021, test_size=0.1\n)\n\n# Create DataLoader\nbatch_size = 16  # Decrease batch size for better generalization\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n# Load RoBERTa model for sequence classification\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"roberta-base\",\n    num_labels=5,\n    output_attentions=False,\n    output_hidden_states=False,\n)\nmodel.to(device)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8, weight_decay=0.01)\nepochs = 6  # Train for more epochs\n\n# Scheduler with warm-up\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=int(len(train_dataloader) * 0.1),\n    num_training_steps=len(train_dataloader) * epochs,\n)\n\n# Accuracy calculation\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Training loop\nfor epoch in trange(epochs, desc=\"Epoch\"):\n    # Training phase\n    model.train()\n    total_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()\n\n        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n\n    # Validation phase\n    model.eval()\n    eval_accuracy = 0\n    nb_eval_steps = 0\n\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.cpu().numpy()\n\n        eval_accuracy += flat_accuracy(logits, label_ids)\n        nb_eval_steps += 1\n\n    print(f\"Validation Accuracy: {eval_accuracy / nb_eval_steps:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T14:45:06.050180Z","iopub.execute_input":"2024-12-23T14:45:06.050467Z","iopub.status.idle":"2024-12-23T15:39:02.145904Z","shell.execute_reply.started":"2024-12-23T14:45:06.050440Z","shell.execute_reply":"2024-12-23T15:39:02.144917Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"554ce626416348c7b22f335894364a7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9c95a5b98442c88d23e163f035fa11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b1e11e8f62042e2a0ef75e6fea0bff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ae0084977f44a9982d328a8ec9ac60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12a38bc969f24ca7bea9d8c1dd852d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8ce30be29f42d3ac5c7e01ea49ebe9"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch:   0%|          | 0/6 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.8174\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  17%|█▋        | 1/6 [08:46<43:54, 526.80s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7561\nAverage Training Loss: 0.5951\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  33%|███▎      | 2/6 [17:45<35:34, 533.59s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7617\nAverage Training Loss: 0.4735\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  50%|█████     | 3/6 [26:43<26:47, 535.67s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7661\nAverage Training Loss: 0.3666\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  67%|██████▋   | 4/6 [35:40<17:52, 536.40s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7569\nAverage Training Loss: 0.2878\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  83%|████████▎ | 5/6 [44:38<08:56, 536.90s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7533\nAverage Training Loss: 0.2379\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|██████████| 6/6 [53:36<00:00, 536.02s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.7557\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nimport pandas as pd\nimport numpy as np\nimport re\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nfrom tqdm import tqdm\n\n# Load the test dataset\ntest_df = pd.read_csv('/kaggle/input/traintest/test.csv')\n\n# Preprocessing\ntest_df['Discussion'] = test_df['Discussion'].astype(str)\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE))\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9.,!?;\\\\'\\\"\\\\s]\", \" \", text))\ntest_df['Discussion'] = test_df['Discussion'].apply(\n    lambda text: ' '.join(filter(lambda word: word.lower() not in stop_words, text.split()))\n)\n\n# Tokenize test sentences\ntest_sentences = test_df['Discussion'].values\ntest_encoded = tokenizer(\n    list(test_sentences),\n    max_length=MAX_LEN,\n    padding='max_length',\n    truncation=True,\n    return_tensors='pt'\n)\n\n# Extract input IDs and attention masks\ntest_input_ids = test_encoded['input_ids']\ntest_attention_masks = test_encoded['attention_mask']\n\n# Prepare the DataLoader for inference\ntest_data = TensorDataset(test_input_ids, test_attention_masks)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n# Perform inference\nmodel.eval()\npredictions = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader, desc=\"Inference\", leave=False):\n        b_input_ids, b_input_mask = tuple(t.to(device) for t in batch)\n\n        # Forward pass\n        outputs = model(b_input_ids, attention_mask=b_input_mask)\n        logits = outputs.logits.detach().cpu().numpy()\n        batch_predictions = np.argmax(logits, axis=1)\n        predictions.extend(batch_predictions)\n\n# Create submission DataFrame\nsubmit = pd.DataFrame({\n    \"SampleID\": test_df[\"SampleID\"],  # Ensure SampleID column exists in test_df\n    \"Category\": predictions\n})\n\n# Save the submission to a CSV file\nsubmit.to_csv(\"/kaggle/working/disrobta3.csv\", index=False)\nprint(submit.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DistilBERT","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm import trange\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\n\n# Preprocessing\ndf['Discussion'].dropna(inplace=True)\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Discussion'] = df['Discussion'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', ' ', x))\ndf['Category'] = df['Category'].map(category_mapping)\ndf = df.drop('SampleID', axis=1)\nstop_words = set(stopwords.words('english'))\ndf['Discussion'] = df['Discussion'].apply(\n    lambda text: ' '.join(filter(lambda word: word.lower() not in stop_words, text.split()))\n)\n\nsentences = df.Discussion.values\nlabels = df.Category.values\n\n# Tokenization using DistilBertTokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize the sentences and pad sequences\nMAX_LEN = 100\ninput_ids = []\nattention_masks = []\n\nfor sent in sentences:\n    encoded_dict = tokenizer.encode_plus(\n        sent,\n        add_special_tokens=True,  # Add [CLS] and [SEP]\n        max_length=MAX_LEN,       # Pad or truncate\n        padding='max_length',\n        return_attention_mask=True,\n        return_tensors='pt',      # Return pytorch tensors\n        truncation=True\n    )\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Split the dataset into training and validation\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, labels, random_state=2021, test_size=0.1\n)\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks, input_ids, random_state=2021, test_size=0.1\n)\n\n# Create DataLoader\nbatch_size = 32\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n# Load DistilBERT model for sequence classification\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=5,  # Number of categories\n    output_attentions=False,\n    output_hidden_states=False,\n)\nmodel.to(device)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nfrom transformers import get_scheduler\n\nepochs = 4\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps,\n)\n\n# Accuracy calculation\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Training loop\nfor epoch in trange(epochs, desc=\"Epoch\"):\n    # Training phase\n    model.train()\n    total_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()\n\n        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n\n    # Validation phase\n    model.eval()\n    eval_accuracy = 0\n    nb_eval_steps = 0\n\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.cpu().numpy()\n\n        eval_accuracy += flat_accuracy(logits, label_ids)\n        nb_eval_steps += 1\n\n    print(f\"Validation Accuracy: {eval_accuracy / nb_eval_steps:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RoBERTa Large","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm import trange\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\n\n# Preprocessing\ndf['Discussion'].dropna(inplace=True)\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Discussion'] = df['Discussion'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', ' ', x))\ndf['Category'] = df['Category'].map(category_mapping)\ndf = df.drop('SampleID', axis=1)\nstop_words = set(stopwords.words('english'))\ndf['Discussion'] = df['Discussion'].apply(\n    lambda text: ' '.join(filter(lambda word: word.lower() not in stop_words, text.split()))\n)\n\nsentences = df.Discussion.values\nlabels = df.Category.values\n\n# Tokenization using RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n\n# Tokenize the sentences and pad sequences\nMAX_LEN = 100\ninput_ids = []\nattention_masks = []\n\nfor sent in sentences:\n    encoded_dict = tokenizer.encode_plus(\n        sent,\n        add_special_tokens=True,  # Add [CLS] and [SEP]\n        max_length=MAX_LEN,       # Pad or truncate\n        padding='max_length',\n        return_attention_mask=True,\n        return_tensors='pt',      # Return pytorch tensors\n        truncation=True\n    )\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Split the dataset into training and validation\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, labels, random_state=2021, test_size=0.1\n)\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks, input_ids, random_state=2021, test_size=0.1\n)\n\n# Create DataLoader\nbatch_size = 32\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n# Load RoBERTa model for sequence classification\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"roberta-large\",\n    num_labels=5,  # Number of categories\n    output_attentions=False,\n    output_hidden_states=False,\n)\nmodel.to(device)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nfrom transformers import get_scheduler\n\nepochs = 4\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps,\n)\n\n# Accuracy calculation\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Training loop\nfor epoch in trange(epochs, desc=\"Epoch\"):\n    # Training phase\n    model.train()\n    total_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()\n\n        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n\n    # Validation phase\n    model.eval()\n    eval_accuracy = 0\n    nb_eval_steps = 0\n\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.cpu().numpy()\n\n        eval_accuracy += flat_accuracy(logits, label_ids)\n        nb_eval_steps += 1\n\n    print(f\"Validation Accuracy: {eval_accuracy / nb_eval_steps:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testsubmission for BERT optimized","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/traintest/test.csv')\ntest_df['Discussion']=test_df['Discussion'].astype(str)\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', ' ', x))\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: ' '.join(filter(lambda word: word.lower() not in stop_words, text.split())))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_sentences = test_df['Discussion'].values\ntest_encoded = tokenizer(\n    list(test_sentences),\n    max_length=MAX_LEN,\n    padding='max_length',\n    truncation=True,\n    return_tensors='pt'\n)\n\n# Extract input IDs and attention masks\ntest_input_ids = test_encoded['input_ids']\ntest_attention_masks = test_encoded['attention_mask']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = TensorDataset(test_input_ids, test_attention_masks)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Perform inference\npredictions = []\nwith torch.no_grad():\n    for batch in test_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask = batch\n\n        # Forward pass\n        outputs = model(b_input_ids, attention_mask=b_input_mask)\n        logits = outputs.logits.detach().cpu().numpy()\n        batch_predictions = np.argmax(logits, axis=1)\n        predictions.extend(batch_predictions)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(predictions)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit = pd.DataFrame({\n    \"SampleID\": test_df[\"SampleID\"],  \n    \"Category\": predictions           \n})\nprint(submit.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit.to_csv(\"/kaggle/working/rot.csv\", index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing\ndf['Discussion'].dropna(inplace=True)\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9.,!?;\\\\'\\\"\\\\s]\", \" \", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"\\d+\", \"<NUMBER>\", text))\ndf['Category'] = df['Category'].map(category_mapping)\ndf = df.drop('SampleID', axis=1)\n# stop_words = set(stopwords.words('english'))\n# df['Discussion'] = df['Discussion'].apply(\n#     lambda text: ' '.join(filter(lambda word: word.lower() not in stop_words, text.split()))\n# )\nfrom transformers import get_scheduler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nsentences = df.Discussion.values\nlabels = df.Category.values\n\n# Tokenization using RobertaTokenizer\nfrom transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # Using 'roberta-base' for efficiency\n\n# Tokenize the sentences and pad sequences\nMAX_LEN = 128\ninput_ids = []\nattention_masks = []\n\nfor sent in sentences:\n    encoded_dict = tokenizer.encode_plus(\n        sent,\n        add_special_tokens=True,  # Add [CLS] and [SEP]\n        max_length=MAX_LEN,       # Pad or truncate\n        padding='max_length',\n        return_attention_mask=True,\n        return_tensors='pt',      # Return pytorch tensors\n        truncation=True\n    )\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Split the dataset into training and validation\nfrom sklearn.model_selection import train_test_split\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, labels, random_state=2021, test_size=0.1\n)\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks, input_ids, random_state=2021, test_size=0.1\n)\n\n# Create DataLoader\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nbatch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n# Ensemble Models\nfrom transformers import RobertaForSequenceClassification\nimport numpy as np\n\n\nmodel_names = ['roberta-base', 'distilbert-base-uncased']\nmodels = []\n\n# Training hyperparameters\nepochs = 3\nlearning_rate = 2e-5\nbatch_size = 32\n\n# Train each model\nfor model_name in model_names:\n    print(f\"Training model: {model_name}\")\n    model = RobertaForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=5,  # Number of categories\n        output_attentions=False,\n        output_hidden_states=False\n    )\n    model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_scheduler(\n        \"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=total_steps\n    )\n\n    # Training loop\n    for epoch in trange(epochs, desc=f\"Epochs for {model_name}\"):\n        model.train()\n        total_loss = 0\n\n        for step, batch in enumerate(train_dataloader):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            model.zero_grad()\n\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs.loss\n            logits = outputs.logits\n            total_loss += loss.item()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_loss / len(train_dataloader)\n        print(f\"Average Training Loss for {model_name}: {avg_train_loss:.4f}\")\n\n    models.append(model)\n\n# Generate predictions for each model\ndef get_model_predictions(model, dataloader):\n    model.eval()\n    all_logits = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n            all_logits.append(logits.cpu().numpy())\n\n    return np.concatenate(all_logits, axis=0)\n\nensemble_logits = []\nfor model in models:\n    logits = get_model_predictions(model, validation_dataloader)\n    ensemble_logits.append(logits)\n\n# Average the logits for soft voting\naverage_logits = np.mean(ensemble_logits, axis=0)\n\n# Get final predictions\nensemble_predictions = np.argmax(average_logits, axis=1)\n\n# Calculate validation accuracy\nlabel_ids = validation_labels.numpy()\naccuracy = np.sum(ensemble_predictions == label_ids) / len(label_ids)\nprint(f\"Ensemble Validation Accuracy: {accuracy:.4f}\") \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"# Preprocessing\ndf['Discussion'].dropna(inplace=True)\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9.,!?;\\\\'\\\"\\\\s]\", \" \", text))\ndf['Discussion'] = df['Discussion'].apply(lambda text: re.sub(r\"\\d+\", \"<NUMBER>\", text))\ndf['Category'] = df['Category'].map(category_mapping)\ndf = df.drop('SampleID', axis=1)\n\nfrom transformers import get_scheduler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nsentences = df.Discussion.values\nlabels = df.Category.values\n\n# Tokenization using RobertaTokenizer\nfrom transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')  # Using 'roberta-base' for efficiency\n\n# Tokenize the sentences and pad sequences\nMAX_LEN = 512\ninput_ids = []\nattention_masks = []\n\nfor sent in sentences:\n    encoded_dict = tokenizer.encode_plus(\n        sent,\n        add_special_tokens=True,  # Add [CLS] and [SEP]\n        max_length=MAX_LEN,       # Pad or truncate\n        padding='max_length',\n        return_attention_mask=True,\n        return_tensors='pt',      # Return pytorch tensors\n        truncation=True\n    )\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Split the dataset into training and validation\nfrom sklearn.model_selection import train_test_split\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids, labels, random_state=2021, test_size=0.1\n)\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks, input_ids, random_state=2021, test_size=0.1\n)\n\n# Create DataLoader\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nbatch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n# Ensemble Models\nfrom transformers import RobertaForSequenceClassification, BertForSequenceClassification\nimport numpy as np\n\nmodel_names = ['roberta-base', 'distilroberta-base']\nmodels = []\n\n# Training hyperparameters\nepochs = 3\nlearning_rate = 2e-5\nbatch_size = 32\nfor model_name in model_names:\n    print(f\"Training model: {model_name}\")\n    model = RobertaForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=5,\n        output_attentions=False,\n        output_hidden_states=False\n    )\n    model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_scheduler(\n        \"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=total_steps\n    )\n\n    # Training loop without class weights\n    for epoch in trange(epochs, desc=f\"Epochs for {model_name}\"):\n        model.train()\n        total_loss = 0\n\n        for step, batch in enumerate(train_dataloader):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            model.zero_grad()\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_loss / len(train_dataloader)\n        print(f\"Average Training Loss for {model_name}: {avg_train_loss:.4f}\")\n\n    models.append(model)\n\n# Generate predictions for ensemble\ndef get_model_predictions(model, dataloader):\n    model.eval()\n    all_logits = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n            all_logits.append(logits.cpu().numpy())\n\n    return np.concatenate(all_logits, axis=0)\n\nensemble_logits = []\nfor model in models:\n    logits = get_model_predictions(model, validation_dataloader)\n    ensemble_logits.append(logits)\n\n# Weighted average for soft voting\nweights = [1.0, 0.8]  # Assign higher weight to the better-performing model\naverage_logits = np.average(ensemble_logits, axis=0, weights=weights)\n\n# Get final predictions\nensemble_predictions = np.argmax(average_logits, axis=1)\n\n# Calculate validation accuracy\nlabel_ids = validation_labels.numpy()\naccuracy = np.sum(ensemble_predictions == label_ids) / len(label_ids)\nprint(f\"Ensemble Validation Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:10:18.797490Z","iopub.execute_input":"2024-12-22T20:10:18.798157Z","iopub.status.idle":"2024-12-22T23:05:39.158401Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f032ba12fdb4c5c98f3b5383851acd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d7a457772dd4bf5baf86c159455f7c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4974560284b74c448242c19a5ad5bad4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33003b9532544e28ad182c441e7e2358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e9754c809ae49dc82ae7818407fd74f"}},"metadata":{}},{"name":"stdout","text":"Training model: roberta-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8488ff1cfc504d9bab45cbb316064e19"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpochs for roberta-base:  33%|███▎      | 1/3 [38:12<1:16:24, 2292.34s/it]","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for roberta-base: 0.7899\n","output_type":"stream"},{"name":"stderr","text":"Epochs for roberta-base:  67%|██████▋   | 2/3 [1:16:38<38:20, 2300.65s/it]","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for roberta-base: 0.5907\n","output_type":"stream"},{"name":"stderr","text":"Epochs for roberta-base: 100%|██████████| 3/3 [1:55:04<00:00, 2301.38s/it]","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for roberta-base: 0.4908\nTraining model: distilroberta-base\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"088978fe81e14ac8a772efacb80261cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"355486cbbc15472bb20901c33fe11269"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpochs for distilroberta-base:  33%|███▎      | 1/3 [19:16<38:33, 1156.54s/it]","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for distilroberta-base: 0.7836\n","output_type":"stream"},{"name":"stderr","text":"Epochs for distilroberta-base:  67%|██████▋   | 2/3 [38:34<19:17, 1157.27s/it]","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for distilroberta-base: 0.6130\n","output_type":"stream"},{"name":"stderr","text":"Epochs for distilroberta-base: 100%|██████████| 3/3 [57:51<00:00, 1157.27s/it]","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for distilroberta-base: 0.5316\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Ensemble Validation Accuracy: 0.7703\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Preprocess the test dataset\ntest_df = pd.read_csv('/kaggle/input/traintest/test.csv')\ntest_df['Discussion'] = test_df['Discussion'].astype(str)\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE))\n#test_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"<.*?>\", \"\", text))\n#test_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9.,!?;\\\\'\\\"\\\\s]\", \" \", text))\ntest_df['Discussion'] = test_df['Discussion'].apply(lambda text: re.sub(r\"\\d+\", \"<NUMBER>\", text))\n\n# Tokenize the test sentences\ntest_sentences = test_df['Discussion'].values\ntest_encoded = tokenizer(\n    list(test_sentences),\n    max_length=MAX_LEN,\n    padding='max_length',\n    truncation=True,\n    return_tensors='pt'\n)\n\n# Extract input IDs and attention masks\ntest_input_ids = test_encoded['input_ids']\ntest_attention_masks = test_encoded['attention_mask']\n\n# Create DataLoader for test data\ntest_data = TensorDataset(test_input_ids, test_attention_masks)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n# Generate predictions for each model in the ensemble\nensemble_logits_test = []\n\nfor model in models:\n    model.eval()\n    all_logits = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n            all_logits.append(logits.cpu().numpy())\n\n    ensemble_logits_test.append(np.concatenate(all_logits, axis=0))\n\n# Average logits for soft voting\naverage_logits_test = np.mean(ensemble_logits_test, axis=0)\n\n# Get final predictions\ntest_predictions = np.argmax(average_logits_test, axis=1)\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test_predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submit = pd.DataFrame({\n    \"SampleID\": test_df[\"SampleID\"],  # Ensure 'SampleID' exists in test_df\n    \"Category\": test_predictions  # Predictions as category labels\n})\n\n# Display the first few rows of the submission DataFrame\nprint(submit.head())\n\n# Save to a CSV file\nsubmit.to_csv(\"/kaggle/working/trial5.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing\ndf=df.dropna()\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Category'] = df['Category'].map(category_mapping)\ndf = df.drop('SampleID', axis=1)\n\nfrom transformers import RobertaTokenizer, BertTokenizer\nimport torch\n\n# Tokenizers\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenization function\ndef tokenize_texts(tokenizer, texts, max_len):\n    input_ids = []\n    attention_masks = []\n    \n    for sent in texts:\n        encoded_dict = tokenizer.encode_plus(\n            sent,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n            truncation=True\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n\nMAX_LEN = 128\nsentences = df.Discussion.values\nlabels = torch.tensor(df.Category.values)\n\n# Tokenize for both models\nroberta_input_ids, roberta_attention_masks = tokenize_texts(roberta_tokenizer, sentences, MAX_LEN)\nbert_input_ids, bert_attention_masks = tokenize_texts(bert_tokenizer, sentences, MAX_LEN)\n\n# Split the dataset\nfrom sklearn.model_selection import train_test_split\n\ndef split_data(input_ids, attention_masks, labels):\n    train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n        input_ids, labels, random_state=2021, test_size=0.1\n    )\n    train_masks, val_masks, _, _ = train_test_split(\n        attention_masks, input_ids, random_state=2021, test_size=0.1\n    )\n    return train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels\n\n# Split for RoBERTa\nroberta_train_inputs, roberta_val_inputs, roberta_train_masks, roberta_val_masks, roberta_train_labels, roberta_val_labels = split_data(\n    roberta_input_ids, roberta_attention_masks, labels\n)\n\n# Split for BERT\nbert_train_inputs, bert_val_inputs, bert_train_masks, bert_val_masks, bert_train_labels, bert_val_labels = split_data(\n    bert_input_ids, bert_attention_masks, labels\n)\n\nfrom transformers import RobertaForSequenceClassification, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom transformers import get_scheduler\nfrom tqdm import trange\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 32\nepochs = 4\nlearning_rate = 2e-5\n\n# Function to train a model\ndef train_model(model, train_dataloader, optimizer, scheduler):\n    model.to(device)\n    model.train()\n    \n    for epoch in trange(epochs, desc=\"Epochs\"):\n        total_loss = 0\n        \n        for batch in train_dataloader:\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            model.zero_grad()\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n        print(f\"Epoch Loss: {total_loss / len(train_dataloader):.4f}\")\n    return model\n\n# Train RoBERTa\nroberta_train_data = TensorDataset(roberta_train_inputs, roberta_train_masks, roberta_train_labels)\nroberta_train_dataloader = DataLoader(roberta_train_data, sampler=RandomSampler(roberta_train_data), batch_size=batch_size)\n\nroberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\noptimizer = AdamW(roberta_model.parameters(), lr=learning_rate, eps=1e-8)\nscheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(roberta_train_dataloader) * epochs)\nroberta_model = train_model(roberta_model, roberta_train_dataloader, optimizer, scheduler)\n\n# Train BERT\nbert_train_data = TensorDataset(bert_train_inputs, bert_train_masks, bert_train_labels)\nbert_train_dataloader = DataLoader(bert_train_data, sampler=RandomSampler(bert_train_data), batch_size=batch_size)\n\nbert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\noptimizer = AdamW(bert_model.parameters(), lr=learning_rate, eps=1e-8)\nscheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(bert_train_dataloader) * epochs)\nbert_model = train_model(bert_model, bert_train_dataloader, optimizer, scheduler)\n# Function to get predictions\ndef get_predictions(model, dataloader):\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n            predictions.append(logits.cpu().numpy())\n\n    return np.concatenate(predictions, axis=0)\n\n# Validation dataloaders\nroberta_val_data = TensorDataset(roberta_val_inputs, roberta_val_masks, roberta_val_labels)\nroberta_val_dataloader = DataLoader(roberta_val_data, sampler=SequentialSampler(roberta_val_data), batch_size=batch_size)\n\nbert_val_data = TensorDataset(bert_val_inputs, bert_val_masks, bert_val_labels)\nbert_val_dataloader = DataLoader(bert_val_data, sampler=SequentialSampler(bert_val_data), batch_size=batch_size)\n\n# Get logits\nroberta_logits = get_predictions(roberta_model, roberta_val_dataloader)\nbert_logits = get_predictions(bert_model, bert_val_dataloader)\n\n# Weighted average\nweights = [1.0, 0.5]  # Adjust based on performance\nensemble_logits = np.average([roberta_logits, bert_logits], axis=0, weights=weights)\nensemble_predictions = np.argmax(ensemble_logits, axis=1)\n\n# Calculate accuracy\nlabel_ids = roberta_val_labels.numpy()\naccuracy = np.sum(ensemble_predictions == label_ids) / len(label_ids)\nprint(f\"Ensemble Validation Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:35:38.739762Z","iopub.execute_input":"2024-12-23T19:35:38.740130Z","iopub.status.idle":"2024-12-23T21:44:47.238101Z","shell.execute_reply.started":"2024-12-23T19:35:38.740097Z","shell.execute_reply":"2024-12-23T21:44:47.237060Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/210371490.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['Discussion'] = df['Discussion'].astype(str)\n/tmp/ipykernel_23/210371490.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['Category'] = df['Category'].map(category_mapping)\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpochs:  25%|██▌       | 1/4 [15:51<47:34, 951.63s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch Loss: 0.7614\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  50%|█████     | 2/4 [31:46<31:46, 953.46s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch Loss: 0.5583\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  75%|███████▌  | 3/4 [47:41<15:54, 954.14s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch Loss: 0.4447\n","output_type":"stream"},{"name":"stderr","text":"Epochs: 100%|██████████| 4/4 [1:03:36<00:00, 954.04s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch Loss: 0.3642\n","output_type":"stream"},{"name":"stderr","text":"\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpochs:  25%|██▌       | 1/4 [15:52<47:37, 952.46s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch Loss: 0.7670\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  50%|█████     | 2/4 [31:44<31:44, 952.35s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch Loss: 0.5322\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  75%|███████▌  | 3/4 [47:37<15:52, 952.39s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch Loss: 0.3974\n","output_type":"stream"},{"name":"stderr","text":"Epochs: 100%|██████████| 4/4 [1:03:29<00:00, 952.44s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch Loss: 0.3044\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Ensemble Validation Accuracy: 0.7915\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/traintest/test.csv')\ntest_df['Discussion'] = test_df['Discussion'].astype(str)\n\n# Tokenize test data for RoBERTa\nroberta_test_input_ids, roberta_test_attention_masks = tokenize_texts(\n    roberta_tokenizer, test_df['Discussion'].values, MAX_LEN\n)\n\n# Tokenize test data for BERT\nbert_test_input_ids, bert_test_attention_masks = tokenize_texts(\n    bert_tokenizer, test_df['Discussion'].values, MAX_LEN\n)\n\n# Create dataloaders\nroberta_test_data = TensorDataset(roberta_test_input_ids, roberta_test_attention_masks)\nroberta_test_dataloader = DataLoader(roberta_test_data, sampler=SequentialSampler(roberta_test_data), batch_size=batch_size)\n\nbert_test_data = TensorDataset(bert_test_input_ids, bert_test_attention_masks)\nbert_test_dataloader = DataLoader(bert_test_data, sampler=SequentialSampler(bert_test_data), batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T21:50:10.339772Z","iopub.execute_input":"2024-12-23T21:50:10.340011Z","iopub.status.idle":"2024-12-23T21:50:30.307071Z","shell.execute_reply.started":"2024-12-23T21:50:10.339987Z","shell.execute_reply":"2024-12-23T21:50:30.306273Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Get predictions for the test data\nroberta_test_logits = get_predictions(roberta_model, roberta_test_dataloader)\nbert_test_logits = get_predictions(bert_model, bert_test_dataloader)\n\n# Weighted average of logits\ntest_ensemble_logits = np.average([roberta_test_logits, bert_test_logits], axis=0, weights=weights)\n\n# Final predictions\ntest_predictions = np.argmax(test_ensemble_logits, axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T21:50:30.308141Z","iopub.execute_input":"2024-12-23T21:50:30.308399Z","iopub.status.idle":"2024-12-23T21:55:32.445935Z","shell.execute_reply.started":"2024-12-23T21:50:30.308375Z","shell.execute_reply":"2024-12-23T21:55:32.445091Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"print(test_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T21:55:32.447183Z","iopub.execute_input":"2024-12-23T21:55:32.447599Z","iopub.status.idle":"2024-12-23T21:55:32.453318Z","shell.execute_reply.started":"2024-12-23T21:55:32.447551Z","shell.execute_reply":"2024-12-23T21:55:32.452275Z"}},"outputs":[{"name":"stdout","text":"[3 0 1 ... 3 0 2]\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"submit = pd.DataFrame({\n    \"SampleID\": test_df[\"SampleID\"],  # Ensure 'SampleID' exists in test_df\n    \"Category\": test_predictions  # Predictions as category labels\n})\n\n# Display the first few rows of the submission DataFrame\nprint(submit.head())\n\n# Save to a CSV file\nsubmit.to_csv(\"finalfinal5.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T21:55:32.454566Z","iopub.execute_input":"2024-12-23T21:55:32.454870Z","iopub.status.idle":"2024-12-23T21:55:32.475400Z","shell.execute_reply.started":"2024-12-23T21:55:32.454841Z","shell.execute_reply":"2024-12-23T21:55:32.474396Z"}},"outputs":[{"name":"stdout","text":"   SampleID  Category\n0         1         3\n1         2         0\n2         3         1\n3         4         4\n4         5         3\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"len(submit)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T21:55:32.476689Z","iopub.execute_input":"2024-12-23T21:55:32.477098Z","iopub.status.idle":"2024-12-23T21:55:32.483112Z","shell.execute_reply.started":"2024-12-23T21:55:32.477056Z","shell.execute_reply":"2024-12-23T21:55:32.482253Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"10557"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"df=df.dropna()\ncategory_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ndf['Discussion'] = df['Discussion'].astype(str)\ndf['Category'] = df['Category'].map(category_mapping)\ndf=df.drop('SampleID',axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:21:12.448207Z","iopub.execute_input":"2024-12-24T20:21:12.448800Z","iopub.status.idle":"2024-12-24T20:21:12.465857Z","shell.execute_reply.started":"2024-12-24T20:21:12.448765Z","shell.execute_reply":"2024-12-24T20:21:12.464784Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/1169491493.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['Discussion'] = df['Discussion'].astype(str)\n/tmp/ipykernel_23/1169491493.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['Category'] = df['Category'].map(category_mapping)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Embedding, MultiHeadAttention, Dense, Dropout, LayerNormalization\nfrom tensorflow.keras.models import Model\n\nclass PositionEmbeddingLayer(Layer):\n    def __init__(self, max_length, d_model):\n        super(PositionEmbeddingLayer, self).__init__()\n        self.max_length = max_length\n        self.d_model = d_model\n        self.position_embeddings = Embedding(input_dim=self.max_length, output_dim=self.d_model)\n\n    def call(self, inputs):\n        seq_len = tf.shape(inputs)[1]\n        position_ids = tf.range(start=0, limit=seq_len, delta=1)\n        position_embeddings = self.position_embeddings(position_ids)\n        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n        position_embeddings = tf.tile(position_embeddings, [tf.shape(inputs)[0], 1, 1])\n        return position_embeddings\n\n\ndef bert_encoder_block(x, d_model, n_heads, d_ff, dropout=0.1, attention_mask=None):\n    attention = MultiHeadAttention(\n        num_heads=n_heads,\n        key_dim=d_model // n_heads,\n        dropout=dropout\n    )(x, x, x, attention_mask=attention_mask)\n    attention = Dropout(dropout)(attention)\n    out1 = LayerNormalization()(x + attention)\n    \n    ffn_output = Dense(d_ff, activation='gelu')(out1)\n    ffn_output = Dense(d_model)(ffn_output)\n    ffn_output = Dropout(dropout)(ffn_output)\n    \n    return LayerNormalization()(out1 + ffn_output)\n\ndef build_bert_for_sequence_classification(n_layers, d_model, n_heads, d_ff, vocab_size, max_length, num_classes, dropout=0.1):\n    # Input layers\n    input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n    token_type_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\")\n    attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n    \n    # Embeddings\n    word_embeddings = Embedding(vocab_size, d_model)(input_ids)\n    position_embeddings = PositionEmbeddingLayer(max_length, d_model)(input_ids)\n    token_type_embeddings = Embedding(2, d_model)(token_type_ids)\n    \n    x = word_embeddings + position_embeddings + token_type_embeddings\n    x = LayerNormalization()(x)\n    x = Dropout(dropout)(x)\n    \n    # Transformer blocks\n    for _ in range(n_layers):\n        x = bert_encoder_block(x, d_model, n_heads, d_ff, dropout, attention_mask)\n    \n    # Classification head\n    cls_token_output = x[:, 0, :]\n    x = Dropout(dropout)(cls_token_output)\n    logits = Dense(num_classes)(x)\n    \n    # Create model\n    model = Model(\n        inputs=[input_ids, token_type_ids, attention_mask],\n        outputs=logits,\n        name=\"BERT_Sequence_Classifier\"\n    )\n    return model\n# Initialize parameters\nMAX_LEN = 128\nBATCH_SIZE = 32\nN_LAYERS = 12\nD_MODEL = 768\nN_HEADS = 12\nD_FF = 3072\nVOCAB_SIZE = 30522\nNUM_CLASSES = 5\nDROPOUT = 0.1\n\n\n# Build model\nmodel1 = build_bert_for_sequence_classification(\n    n_layers=N_LAYERS,\n    d_model=D_MODEL,\n    n_heads=N_HEADS,\n    d_ff=D_FF,\n    vocab_size=VOCAB_SIZE,\n    max_length=MAX_LEN,\n    num_classes=NUM_CLASSES,\n    dropout=DROPOUT\n)\n\n# Print model summary\nmodel1.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:05:31.064427Z","iopub.execute_input":"2024-12-24T22:05:31.065251Z","iopub.status.idle":"2024-12-24T22:05:31.845725Z","shell.execute_reply.started":"2024-12-24T22:05:31.065213Z","shell.execute_reply":"2024-12-24T22:05:31.844961Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"BERT_Sequence_Classifier\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"BERT_Sequence_Classifier\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_52        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │ \u001b[38;5;34m23,440,896\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ position_embedding… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │     \u001b[38;5;34m98,304\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mPositionEmbedding…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_type_ids      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ position_embeddi… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_54        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ token_type_ids[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ embedding_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_407         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ dropout_407[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dropout_407[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ dropout_407[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_409         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout_407[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ dropout_409[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_274 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_275 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_274[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_410         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_275[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_410[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_412         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_412[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_276 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_277 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_276[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_413         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_277[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_9 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_413[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_415         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_10 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_415[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_278 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_279 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_278[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_416         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_279[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_11 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_416[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_418         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_12 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_418[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_280 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_281 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_280[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_419         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_281[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_13 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_419[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_421         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_14 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_421[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_282 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_283 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_282[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_422         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_283[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_15 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_422[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_424         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_16 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_424[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_284 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_285 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_284[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_425         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_285[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_17 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_425[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_427         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_18 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_427[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_286 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_287 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_286[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_428         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_287[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_19 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_428[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_430         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_20 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_430[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_288 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_289 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_288[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_431         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_289[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_21 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_431[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_433         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_22 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_433[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_290 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_291 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_290[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_434         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_291[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_23 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_434[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_436         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_24 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_436[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_292 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_293 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_292[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_437         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_293[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_25 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_437[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_439         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_26 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_439[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_294 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_295 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_294[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_440         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_295[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_27 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_440[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_442         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_28 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_442[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_296 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_297 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_296[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_443         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_297[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_29 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_443[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item_10         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_444         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ get_item_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_298 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │      \u001b[38;5;34m3,845\u001b[0m │ dropout_444[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_52        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">23,440,896</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ position_embedding… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,304</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_type_ids      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ position_embeddi… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_54        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ embedding_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_407         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ dropout_407[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dropout_407[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ dropout_407[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_409         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_407[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ dropout_409[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_274 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_275 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_274[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_410         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_275[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_410[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_412         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_412[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_276 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_277 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_276[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_413         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_277[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_413[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_415         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_415[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_278 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_279 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_278[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_416         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_279[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_416[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_418         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_418[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_280 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_281 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_280[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_419         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_281[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_419[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_421         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_421[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_282 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_283 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_282[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_422         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_283[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_422[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_424         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_424[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_284 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_285 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_284[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_425         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_285[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_425[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_427         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_427[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_286 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_287 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_286[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_428         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_287[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_428[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_430         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_430[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_288 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_289 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_288[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_431         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_289[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_431[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_433         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_433[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_290 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_291 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_290[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_434         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_291[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_434[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_436         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_436[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_292 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_293 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_292[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_437         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_293[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_437[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_439         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_439[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_294 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_295 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_294[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_440         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_295[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_440[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_442         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_442[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_296 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_297 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_296[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_443         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_297[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_443[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item_10         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_444         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ get_item_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_298 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845</span> │ dropout_444[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m108,600,581\u001b[0m (414.28 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">108,600,581</span> (414.28 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m108,600,581\u001b[0m (414.28 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">108,600,581</span> (414.28 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Dropout, LayerNormalization, Lambda\nfrom tensorflow.keras.models import Model\nfrom transformers import RobertaTokenizer\n\n# Parameters\nMAX_LEN = 128  # Using your original max length\nBATCH_SIZE = 32  # Your original batch size\nN_LAYERS = 12\nD_MODEL = 768\nN_HEADS = 12\nD_FF = 3072\nNUM_CLASSES = 5\nDROPOUT = 0.1\n\n# Initialize tokenizer and get vocabulary size\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nVOCAB_SIZE = tokenizer.vocab_size\n\n# Position embedding layer\ndef position_embedding_layer(inputs, max_length, d_model):\n    def compute_position_embeddings(inputs):\n        position_embeddings = Embedding(max_length, d_model)(tf.range(start=0, limit=tf.shape(inputs)[1], delta=1))\n        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n        return tf.tile(position_embeddings, [tf.shape(inputs)[0], 1, 1])\n\n    return Lambda(compute_position_embeddings)(inputs)\n\n# RoBERTa encoder block\ndef roberta_encoder_block(x, d_model, n_heads, d_ff, dropout=0.1, attention_mask=None):\n    attention = MultiHeadAttention(\n        num_heads=n_heads,\n        key_dim=d_model // n_heads,\n        dropout=dropout\n    )(x, x, x, attention_mask=attention_mask)\n    attention = Dropout(dropout)(attention)\n    out1 = LayerNormalization()(x + attention)\n    \n    ffn_output = Dense(d_ff, activation='gelu')(out1)\n    ffn_output = Dense(d_model)(ffn_output)\n    ffn_output = Dropout(dropout)(ffn_output)\n    \n    return LayerNormalization()(out1 + ffn_output)\n\n# Build RoBERTa model for sequence classification\ndef build_roberta_for_sequence_classification(\n    n_layers, d_model, n_heads, d_ff, vocab_size, max_length, num_classes, dropout=0.1\n):\n    # Input layers\n    input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n    \n    # Embeddings\n    word_embeddings = Embedding(vocab_size, d_model)(input_ids)\n    position_embeddings = position_embedding_layer(input_ids, max_length, d_model)\n    x = word_embeddings + position_embeddings\n    x = LayerNormalization()(x)\n    x = Dropout(dropout)(x)\n    \n    # Transformer blocks\n    for _ in range(n_layers):\n        x = roberta_encoder_block(x, d_model, n_heads, d_ff, dropout, attention_mask)\n    \n    # Classification head with two dense layers and tanh activation\n    cls_token_output = x[:, 0, :]\n    x = Dropout(dropout)(cls_token_output)\n    x = Dense(d_model, activation='tanh')(x)  # First Dense layer with tanh activation\n    x = Dropout(dropout)(x)\n    logits = Dense(num_classes)(x)  # Second Dense layer for final output\n    \n    # Create model\n    model = Model(\n        inputs=[input_ids, attention_mask],\n        outputs=logits,\n        name=\"RoBERTa_Sequence_Classifier\"\n    )\n    return model\n\n\n# Build model\nMAX_LEN = 128  # Using your original max length\nBATCH_SIZE = 32  # Your original batch size\nN_LAYERS = 12\nD_MODEL = 768\nN_HEADS = 12\nD_FF = 3072\nNUM_CLASSES = 5\nDROPOUT = 0.1\n\nmodel2 = build_roberta_for_sequence_classification(\n    n_layers=N_LAYERS,\n    d_model=D_MODEL,\n    n_heads=N_HEADS,\n    d_ff=D_FF,\n    vocab_size=VOCAB_SIZE,\n    max_length=MAX_LEN,\n    num_classes=NUM_CLASSES,\n    dropout=DROPOUT\n)\n\n# Print model summary\nmodel2.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T22:28:09.268700Z","iopub.execute_input":"2024-12-24T22:28:09.269523Z","iopub.status.idle":"2024-12-24T22:28:10.231412Z","shell.execute_reply.started":"2024-12-24T22:28:09.269484Z","shell.execute_reply":"2024-12-24T22:28:10.230571Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"RoBERTa_Sequence_Classifier_With_Tanh\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"RoBERTa_Sequence_Classifier_With_Tanh\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_69        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │ \u001b[38;5;34m38,603,520\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_107 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │                   │            │ lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_590         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ dropout_590[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dropout_590[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ dropout_590[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_592         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_108 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dropout_590[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ dropout_592[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_108[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_403 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_404 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_403[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_593         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_404[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_109 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_593[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_109[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_595         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_110 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_595[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_110[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_405 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_406 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_405[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_596         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_406[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_111 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_596[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_111[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_598         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_112 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_598[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_112[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_407 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_408 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_407[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_599         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_408[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_113 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_599[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_113[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_601         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_114 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_601[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_114[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_409 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_410 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_409[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_602         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_410[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_115 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_602[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_115[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_604         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_116 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_604[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_116[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_411 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_412 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_411[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_605         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_412[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_117 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_605[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_117[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_607         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_118 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_607[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_118[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_413 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_414 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_413[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_608         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_414[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_119 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_608[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_119[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_610         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_120 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_610[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_120[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_415 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_416 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_415[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_611         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_416[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_121 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_611[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_613         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_122 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_613[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_122[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_417 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_418 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_417[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_614         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_418[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_123 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_614[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_123[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_616         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_124 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_616[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_124[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_419 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_420 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_419[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_617         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_420[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_125 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_617[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_125[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_619         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_126 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_619[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_126[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_421 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_422 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_421[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_620         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_422[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_127 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_620[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_127[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_622         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_128 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_622[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_423 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_424 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_423[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_623         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_424[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_129 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_623[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_625         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_130 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_625[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_425 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3072\u001b[0m) │  \u001b[38;5;34m2,362,368\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_426 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │  \u001b[38;5;34m2,360,064\u001b[0m │ dense_425[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_626         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_426[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_131 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_626[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │      \u001b[38;5;34m1,536\u001b[0m │ add_131[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item_14         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_627         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ get_item_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_427 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │    \u001b[38;5;34m590,592\u001b[0m │ dropout_627[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_628         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_427[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_428 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │      \u001b[38;5;34m3,845\u001b[0m │ dropout_628[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_69        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38,603,520</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_590         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ dropout_590[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dropout_590[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ dropout_590[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_592         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_590[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ dropout_592[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_108[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_403 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_404 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_403[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_593         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_404[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_593[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_109[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_595         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_595[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_110[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_405 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_406 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_405[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_596         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_406[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_596[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_111[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_598         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_598[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_112[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_407 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_408 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_407[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_599         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_408[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_113 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_599[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_113[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_601         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_601[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_114[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_409 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_410 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_409[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_602         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_410[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_115 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_602[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_115[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_604         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_604[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_116[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_411 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_412 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_411[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_605         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_412[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_605[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_117[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_607         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_607[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_118[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_413 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_414 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_413[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_608         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_414[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_608[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_119[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_610         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_120 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_610[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_120[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_415 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_416 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_415[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_611         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_416[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_611[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_613         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_122 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_613[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_122[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_417 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_418 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_417[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_614         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_418[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_123 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_614[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_123[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_616         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_124 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_616[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_124[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_419 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_420 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_419[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_617         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_420[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_125 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_617[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_125[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_619         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_619[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_126[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_421 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_422 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_421[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_620         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_422[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_620[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_127[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_622         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_622[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_423 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_424 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_423[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_623         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_424[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_623[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n│                     │                   │            │ layer_normalizat… │\n│                     │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_625         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_625[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_425 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,362,368</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_426 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,064</span> │ dense_425[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_626         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_426[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_131 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_626[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ add_131[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item_14         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_627         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ get_item_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_427 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │ dropout_627[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_628         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_427[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_428 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845</span> │ dropout_628[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,253,957\u001b[0m (473.99 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,253,957</span> (473.99 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,253,957\u001b[0m (473.99 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,253,957</span> (473.99 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import BertTokenizer\nimport numpy as np\nfrom tqdm import trange\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef tokenize_and_prepare_data(tokenizer, texts, labels, max_len):\n    \"\"\"\n    Tokenize texts and prepare data for training\n    \"\"\"\n    # Ensure texts are strings\n    texts = [str(text) for text in texts]\n    \n    # Tokenize\n    encodings = tokenizer(\n        texts,\n        add_special_tokens=True,\n        max_length=max_len,\n        padding='max_length',\n        return_attention_mask=True,\n        return_token_type_ids=True,\n        truncation=True,\n        return_tensors='tf'\n    )\n    \n    return encodings['input_ids'], encodings['attention_mask'], encodings['token_type_ids'], tf.convert_to_tensor(labels, dtype=tf.int32)\n\ndef split_data(input_ids, attention_masks, token_type_ids, labels, test_size=0.1):\n    \"\"\"\n    Split the dataset into train and validation sets using TensorFlow operations\n    \"\"\"\n    # Calculate split sizes\n    total_size = input_ids.shape[0]\n    val_size = int(total_size * test_size)\n    train_size = total_size - val_size\n    \n    # Create shuffled indices\n    indices = tf.random.shuffle(tf.range(total_size))\n    \n    # Split indices into train and validation\n    train_indices = indices[:train_size]\n    val_indices = indices[train_size:]\n    \n    # Use tf.gather to split the data\n    train_inputs = tf.gather(input_ids, train_indices)\n    val_inputs = tf.gather(input_ids, val_indices)\n    \n    train_masks = tf.gather(attention_masks, train_indices)\n    val_masks = tf.gather(attention_masks, val_indices)\n    \n    train_token_types = tf.gather(token_type_ids, train_indices)\n    val_token_types = tf.gather(token_type_ids, val_indices)\n    \n    train_labels = tf.gather(labels, train_indices)\n    val_labels = tf.gather(labels, val_indices)\n    \n    return (train_inputs, val_inputs,\n            train_masks, val_masks,\n            train_token_types, val_token_types,\n            train_labels, val_labels)\n\ndef create_tf_dataset(input_ids, attention_masks, token_type_ids, labels, batch_size, is_training=True):\n    \"\"\"\n    Create a TensorFlow dataset\n    \"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices((\n        {\n            'input_ids': input_ids,\n            'attention_mask': attention_masks,\n            'token_type_ids': token_type_ids\n        },\n        labels\n    ))\n    \n    if is_training:\n        dataset = dataset.shuffle(1000)\n    \n    dataset = dataset.batch(batch_size)\n    \n    return dataset\n\ndef prepare_and_train_model(df, model, tokenizer, max_len=128, batch_size=32, epochs=4):\n    \"\"\"\n    Prepare data and train the model\n    \"\"\"\n    # Prepare inputs\n    input_ids, attention_masks, token_type_ids, labels = tokenize_and_prepare_data(\n        tokenizer, \n        df.Discussion.values, \n        df.Category.values, \n        max_len\n    )\n    \n    # Split data using TensorFlow operations\n    (train_inputs, val_inputs,\n     train_masks, val_masks,\n     train_token_types, val_token_types,\n     train_labels, val_labels) = split_data(\n        input_ids, attention_masks, token_type_ids, labels\n    )\n    \n    # Create datasets\n    train_dataset = create_tf_dataset(\n        train_inputs, train_masks, train_token_types, train_labels, \n        batch_size=batch_size, is_training=True\n    )\n    \n    val_dataset = create_tf_dataset(\n        val_inputs, val_masks, val_token_types, val_labels, \n        batch_size=batch_size, is_training=False\n    )\n    \n    # Compile model\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-8),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=['accuracy']\n    )\n    \n    # Train model\n    history = model.fit(\n        train_dataset,\n        epochs=2,\n        validation_data=val_dataset,\n        verbose=1\n    )\n    \n    return model, history, val_dataset\n\n\n# Example usage:\n\"\"\"\n# Assuming df is your preprocessed dataframe with 'Discussion' and 'Category' columns\nmodel, history, val_dataset = prepare_and_train_model(df, model, tokenizer)\n\n# Get predictions\nval_predictions = model.predict(val_dataset)\nval_predictions = np.argmax(val_predictions, axis=1)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:24:31.882483Z","iopub.execute_input":"2024-12-24T20:24:31.883373Z","iopub.status.idle":"2024-12-24T20:24:31.899908Z","shell.execute_reply.started":"2024-12-24T20:24:31.883338Z","shell.execute_reply":"2024-12-24T20:24:31.898935Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"\"\\n# Assuming df is your preprocessed dataframe with 'Discussion' and 'Category' columns\\nmodel, history, val_dataset = prepare_and_train_model(df, model, tokenizer)\\n\\n# Get predictions\\nval_predictions = model.predict(val_dataset)\\nval_predictions = np.argmax(val_predictions, axis=1)\\n\""},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"model, history, val_dataset = prepare_and_train_model(df, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:24:36.042099Z","iopub.execute_input":"2024-12-24T20:24:36.042793Z","iopub.status.idle":"2024-12-24T20:45:23.682902Z","shell.execute_reply.started":"2024-12-24T20:24:36.042759Z","shell.execute_reply":"2024-12-24T20:45:23.681977Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1735071930.733720     100 service.cc:145] XLA service 0x7d59d003dd50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1735071930.733785     100 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1735071930.733789     100 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1735071979.576876     180 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_76', 1348 bytes spill stores, 1348 bytes spill loads\n\nI0000 00:00:1735072024.820282     100 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_99', 48 bytes spill stores, 48 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'copy_fusion_6', 16 bytes spill stores, 16 bytes spill loads\n\nI0000 00:00:1735072024.917306     100 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m693/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.2272 - loss: 1.8439","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1735072556.402590     206 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_469', 8 bytes spill stores, 8 bytes spill loads\n\nI0000 00:00:1735072557.956490     204 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_469', 968 bytes spill stores, 924 bytes spill loads\n\nI0000 00:00:1735072602.374572     100 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_54', 80 bytes spill stores, 80 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_40', 48 bytes spill stores, 48 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_36', 8 bytes spill stores, 8 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 866ms/step - accuracy: 0.2274 - loss: 1.8434 - val_accuracy: 0.4237 - val_loss: 1.3134\nEpoch 2/2\n\u001b[1m694/694\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 718ms/step - accuracy: 0.4762 - loss: 1.2627 - val_accuracy: 0.6351 - val_loss: 0.9915\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"history = model.fit(\n        train_dataset,\n        epochs=3,\n        validation_data=val_dataset,\n        verbose=1\n    )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T20:47:29.037104Z","iopub.execute_input":"2024-12-24T20:47:29.037877Z","iopub.status.idle":"2024-12-24T21:19:57.916440Z","shell.execute_reply.started":"2024-12-24T20:47:29.037843Z","shell.execute_reply":"2024-12-24T21:19:57.915498Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1735073305.316379     271 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_469', 8 bytes spill stores, 8 bytes spill loads\n\nI0000 00:00:1735073306.515848     273 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_493', 8 bytes spill stores, 8 bytes spill loads\n\nI0000 00:00:1735073312.588267     272 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_469', 968 bytes spill stores, 924 bytes spill loads\n\nI0000 00:00:1735073313.134489     271 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_493', 968 bytes spill stores, 924 bytes spill loads\n\nI0000 00:00:1735073354.059686     103 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_68', 80 bytes spill stores, 80 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_55', 48 bytes spill stores, 48 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1540/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 0.6306 - loss: 0.9563","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1735074004.126462     103 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_54', 80 bytes spill stores, 80 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_40', 48 bytes spill stores, 48 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_36', 8 bytes spill stores, 8 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m775s\u001b[0m 435ms/step - accuracy: 0.6306 - loss: 0.9563 - val_accuracy: 0.7244 - val_loss: 0.7476\nEpoch 2/3\n\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 381ms/step - accuracy: 0.7194 - loss: 0.7521 - val_accuracy: 0.7386 - val_loss: 0.7357\nEpoch 3/3\n\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 381ms/step - accuracy: 0.7708 - loss: 0.6293 - val_accuracy: 0.8385 - val_loss: 0.4837\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"history = model.fit(\n        train_dataset,\n        epochs=3,\n        validation_data=val_dataset,\n        verbose=1\n    )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:20:30.490376Z","iopub.execute_input":"2024-12-24T21:20:30.490699Z","iopub.status.idle":"2024-12-24T21:49:48.952490Z","shell.execute_reply.started":"2024-12-24T21:20:30.490671Z","shell.execute_reply":"2024-12-24T21:49:48.951746Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m586s\u001b[0m 381ms/step - accuracy: 0.8122 - loss: 0.5252 - val_accuracy: 0.8636 - val_loss: 0.4085\nEpoch 2/3\n\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 380ms/step - accuracy: 0.8418 - loss: 0.4474 - val_accuracy: 0.8669 - val_loss: 0.3816\nEpoch 3/3\n\u001b[1m1541/1541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 381ms/step - accuracy: 0.8633 - loss: 0.3846 - val_accuracy: 0.9058 - val_loss: 0.2759\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"print(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:50:12.838168Z","iopub.execute_input":"2024-12-24T21:50:12.838509Z","iopub.status.idle":"2024-12-24T21:50:12.843411Z","shell.execute_reply.started":"2024-12-24T21:50:12.838481Z","shell.execute_reply":"2024-12-24T21:50:12.842557Z"}},"outputs":[{"name":"stdout","text":"BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\n# Assuming the model is trained and tokenizer is loaded\n\n# Load the test data\ntest_df = pd.read_csv('/kaggle/input/traintest/test.csv')\ntest_df['Discussion'] = test_df['Discussion'].astype(str)\n\n# Tokenize the test data\ntest_input_ids, test_attention_masks, test_token_type_ids, _ = tokenize_and_prepare_data(\n    tokenizer, \n    test_df.Discussion.values, \n    np.zeros(len(test_df)),  # Placeholder, since we don't have labels\n    max_len=128\n)\n\n# Create the test dataset\ndef create_test_dataset(input_ids, attention_masks, token_type_ids, batch_size):\n    \"\"\"\n    Create a TensorFlow dataset for the test data\n    \"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices((\n        {\n            'input_ids': input_ids,\n            'attention_mask': attention_masks,\n            'token_type_ids': token_type_ids\n        }\n    ))\n    \n    dataset = dataset.batch(batch_size)\n    return dataset\n\ntest_dataset = create_test_dataset(test_input_ids, test_attention_masks, test_token_type_ids, batch_size=32)\n\n# Perform inference\npredictions = []\n\nfor batch in test_dataset:\n    # Extract inputs\n    input_ids = batch['input_ids']\n    attention_masks = batch['attention_mask']\n    token_type_ids = batch['token_type_ids']\n    \n    # Create a dictionary for the model\n    inputs = {\n        'input_ids': input_ids,\n        'attention_mask': attention_masks,\n        'token_type_ids': token_type_ids\n    }\n    \n    # Perform inference by passing the entire dictionary\n    outputs = model(inputs)  # Pass the inputs as a dictionary\n    \n    # If the model is producing a tensor, access the logits directly from the output\n    logits = outputs.numpy()  # Convert the output tensor to numpy for processing\n    batch_predictions = np.argmax(logits, axis=1)  # Get predicted class indices\n    predictions.extend(batch_predictions)\n\n# Create a DataFrame for submission\nsubmit = pd.DataFrame({\n    \"SampleID\": test_df[\"SampleID\"],  \n    \"Category\": predictions           \n})\n\n# Save the predictions to CSV\nsubmit.to_csv(\"/kaggle/working/nicetry.csv\", index=False)\n\nprint(submit.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T21:58:49.303224Z","iopub.execute_input":"2024-12-24T21:58:49.303586Z","iopub.status.idle":"2024-12-24T22:00:39.811676Z","shell.execute_reply.started":"2024-12-24T21:58:49.303555Z","shell.execute_reply":"2024-12-24T22:00:39.810549Z"}},"outputs":[{"name":"stdout","text":"   SampleID  Category\n0         1         3\n1         2         0\n2         3         1\n3         4         4\n4         5         3\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}